import os
import sys
import pathlib
import subprocess
import re
import typing

"""
convert_md_to_pdfs.py
Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’Microsoft Word(docx)ãŠã‚ˆã³PDFã«å¤‰æ›ã™ã‚‹ã€‚
pandocã‚’åˆ©ç”¨ã€‚å¼•æ•°ã¯ <markdown_file|directory|files...> ã®ã¿ã€‚å‡ºåŠ›å…ˆã¯ file=è¦ªã®è¦ªã€dir=è¦ªã®ç›´ä¸‹ã« /docx ãŠã‚ˆã³ /pdf ã‚’ä½œæˆã™ã‚‹ã€‚
ver2.1ã§ add_metadata.py ãŒç”Ÿæˆã—ãŸã‚µã‚¤ãƒ‰ã‚«ãƒ¼YAMLã‹ã‚‰å¼•ç”¨è„šæ³¨ãƒ©ãƒ™ãƒ«ã‚’è‡ªå‹•å–å¾—ã™ã‚‹ã‚ˆã†ã«å¤‰æ›´ã€‚
"""

# ==== è¨­å®š ======================================================
# ç’°å¢ƒå¤‰æ•°ã§ã‚‚ä¸Šæ›¸ãå¯èƒ½ï¼šBLOCKQUOTE_ATTRIBUTION="...your text..."
_ENV_BLOCKQUOTE_ATTRIBUTION = os.environ.get("BLOCKQUOTE_ATTRIBUTION")
if _ENV_BLOCKQUOTE_ATTRIBUTION is not None:
    _ENV_BLOCKQUOTE_ATTRIBUTION = _ENV_BLOCKQUOTE_ATTRIBUTION.strip()
DEFAULT_BLOCKQUOTE_ATTRIBUTION = _ENV_BLOCKQUOTE_ATTRIBUTION or "å¤§å­¦ å¹´åº¦ è©¦é¨“ç§‘ç›®ä¸æ˜"
DOCX_PANDOC_INPUT_FORMAT = "gfm-yaml_metadata_block-raw_html"
# PDFç”Ÿæˆæ™‚ã®Markdownå…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆGFMç›¸å½“ã®æ‹¡å¼µã‚’ä¿æŒã—ã¤ã¤ raw_tex ã‚’è¨±å¯ï¼‰
PDF_PANDOC_INPUT_FORMAT = (
    "markdown"
    "+hard_line_breaks"
    "+yaml_metadata_block"
    "+gfm_auto_identifiers"
    "+pipe_tables"
    "+table_captions"
    "+strikeout"
    "+task_lists"
    "+definition_lists"
    "+fenced_code_blocks"
    "+auto_identifiers"
    "+footnotes"
    "+raw_tex" # è„šæ³¨ã‚’ã¤ã‘ã‚‹ãŸã‚å¿…é ˆã€‚æ¶ˆã—ã¦ã¯ã„ã‘ãªã„
)
# ===============================================================

# v3.6: ç‰¹å®šã®è¨˜å·ã‚’ASCIIã¸ç¢ºå®Ÿã«è½ã¨ã—ã€ã‚µãƒ­ã‚²ãƒ¼ãƒˆãƒšã‚¢é ˜åŸŸã®çµµæ–‡å­—ã‚’é™¤å»ã—ã¦pandoc/pdftexã®è­¦å‘Šã‚’å›é¿
SPECIAL_REPLACEMENTS_V36 = {
    "â˜": "[ ]",  # U+2610
    "â˜‘": "[x]",  # U+2611
    "ğŸ”˜": "(â—)",  # U+1F518
    "âšª": "( )",  # U+26AA
    "â¬œ": "[ ]",  # U+2B1C
}
ASTRAL_RE = re.compile(r"[\U00010000-\U0010FFFF]")


def sanitize_symbols_v36(text: str) -> str:
    for src, dst in SPECIAL_REPLACEMENTS_V36.items():
        text = text.replace(src, dst)
    return ASTRAL_RE.sub("", text)

from pathlib import Path
import logging

logger = logging.getLogger(__name__)

_METADATA_KEYS = ("å¤§å­¦å", "å¹´åº¦", "è©¦é¨“ç§‘ç›®")


def build_blockquote_attribution(data: typing.Mapping[str, typing.Any]) -> str:
    """å¤§å­¦åãƒ»å¹´åº¦ãƒ»è©¦é¨“ç§‘ç›®ã‹ã‚‰è„šæ³¨ãƒ©ãƒ™ãƒ«ã‚’ç”Ÿæˆã€‚æ¬ æã¯ã€Œä¸æ˜ã€ã§è£œã†ã€‚"""
    parts: list[str] = []
    for key in _METADATA_KEYS:
        raw = data.get(key, "") if hasattr(data, "get") else ""
        if raw is None:
            text = ""
        elif isinstance(raw, str):
            text = raw.strip()
        else:
            text = str(raw).strip()
        parts.append(text or "ä¸æ˜")
    return " ".join(parts)


def _find_metadata_yaml(md_path: Path) -> typing.Optional[Path]:
    """å¯¾è±¡Markdownã«å¯¾å¿œã™ã‚‹ã‚µã‚¤ãƒ‰ã‚«ãƒ¼YAMLã‚’æ¢ç´¢ã™ã‚‹ã€‚"""
    base_name = f"{md_path.stem}_metadata.yaml"
    candidates: list[Path] = []
    seen: set[Path] = set()
    for parent in [md_path.parent, *md_path.parents]:
        for candidate in (parent / base_name, parent / "metadata-yaml" / base_name):
            if candidate in seen:
                continue
            seen.add(candidate)
            candidates.append(candidate)
    for candidate in candidates:
        if candidate.is_file():
            return candidate
    return None


def _load_metadata_from_yaml(yaml_path: Path) -> dict[str, typing.Any]:
    """ã‚µã‚¤ãƒ‰ã‚«ãƒ¼YAMLã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ï¼ˆPyYAMLãŒç„¡ã„å ´åˆã¯ç°¡æ˜“è§£æï¼‰ã€‚"""
    try:
        text = yaml_path.read_text(encoding="utf-8")
    except Exception as exc:
        logger.warning("metadata èª­ã¿è¾¼ã¿ã«å¤±æ•—: %s (%s)", yaml_path, exc)
        return {}

    data: dict[str, typing.Any] = {}
    try:
        import yaml  # type: ignore
    except Exception:
        yaml = None

    if yaml is not None:
        try:
            loaded = yaml.safe_load(text)
            if isinstance(loaded, dict):
                data = loaded
        except Exception as exc:
            logger.debug("PyYAMLã§ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: %s (%s)", yaml_path, exc)

    if not data:
        for line in text.splitlines():
            if ":" not in line:
                continue
            stripped = line.strip()
            if not stripped or stripped.startswith("#"):
                continue
            key, value = line.split(":", 1)
            key = key.strip()
            if key in _METADATA_KEYS:
                data[key] = value.strip()

    return data


def resolve_blockquote_attribution(md_path: Path) -> tuple[str, str, typing.Optional[Path], list[str]]:
    """
    BLOCKQUOTE_ATTRIBUTION ã‚’æ±ºå®šã™ã‚‹ã€‚
    å„ªå…ˆé †ä½: ç’°å¢ƒå¤‰æ•° > ã‚µã‚¤ãƒ‰ã‚«ãƒ¼YAML > æ—¢å®šå€¤ï¼ˆå…¨ã¦ä¸æ˜ï¼‰ã€‚
    æˆ»ã‚Šå€¤ã¯ (è„šæ³¨ãƒ†ã‚­ã‚¹ãƒˆ, ã‚½ãƒ¼ã‚¹ç¨®åˆ¥, ä½¿ç”¨ã—ãŸYAMLãƒ‘ã‚¹, ä¸æ˜æ‰±ã„ã‚­ãƒ¼)ã€‚
    """
    if _ENV_BLOCKQUOTE_ATTRIBUTION:
        return DEFAULT_BLOCKQUOTE_ATTRIBUTION, "environment", None, []

    yaml_path = _find_metadata_yaml(md_path)
    if yaml_path:
        metadata = _load_metadata_from_yaml(yaml_path)
        missing: list[str] = []
        for key in _METADATA_KEYS:
            raw = metadata.get(key)
            if isinstance(raw, str):
                text = raw.strip()
            elif raw is None:
                text = ""
            else:
                text = str(raw).strip()
            if not text:
                missing.append(key)
        attr = build_blockquote_attribution(metadata)
        return attr, "metadata", yaml_path, missing

    return DEFAULT_BLOCKQUOTE_ATTRIBUTION, "fallback", None, list(_METADATA_KEYS)


def _inject_attribution_to_blockquotes(md_text: str, attribution_text: str) -> str:
    """
    Markdownãƒ†ã‚­ã‚¹ãƒˆå†…ã® blockquoteï¼ˆå…ˆé ­ãŒ '>'ï¼‰ã®â€œå„å¡Šâ€ã®æœ«å°¾ã«ã€
    æŒ‡å®šã®è„šæ³¨ï¼ˆTeXï¼‰ã‚’ raw_tex ã¨ã—ã¦è¿½è¨˜ã™ã‚‹ã€‚
    ã™ã§ã« \begin{flushright} ã‚„ attribution æœ¬æ–‡ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯äºŒé‡ä»˜ä¸ã‚’é¿ã‘ã‚‹ã€‚
    """
    lines = md_text.splitlines(keepends=False)
    out: list[str] = []
    i = 0
    n = len(lines)

    # è¿½è¨˜ã™ã‚‹ quoted TeX ã‚¹ãƒ‹ãƒšãƒƒãƒˆï¼ˆblockquote å†…ã«å…¥ã‚Œã‚‹ã®ã§ '>' ã‚’ä»˜ä¸ï¼‰
    def quoted_snippet(attr: str) -> list[str]:
        return [
            ">",
            "> \\par\\vspace{0.8\\baselineskip}",
            "> \\begin{flushright}\\footnotesize",
            f"> --- {attr}",
            "> \\end{flushright}",
        ]

    while i < n:
        line = lines[i]
        # blockquote ã®é–‹å§‹åˆ¤å®šï¼šç©ºç™½â†’'>' ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚è¨±å®¹
        if line.lstrip().startswith(">"):
            # blockquote ãƒ–ãƒ­ãƒƒã‚¯ã‚’åé›†
            block: list[str] = []
            while i < n and lines[i].lstrip().startswith(">"):
                block.append(lines[i])
                i += 1

            # æœ«å°¾é‡è¤‡é˜²æ­¢ãƒã‚§ãƒƒã‚¯ï¼ˆã™ã§ã«è„šæ³¨ã£ã½ã„ã‚‚ã®ãŒã‚ã‚‹ã‹ï¼‰
            joined_tail = "\n".join(block[-10:]) if block else ""
            existing_markers = [attribution_text]
            if DEFAULT_BLOCKQUOTE_ATTRIBUTION and DEFAULT_BLOCKQUOTE_ATTRIBUTION not in existing_markers:
                existing_markers.append(DEFAULT_BLOCKQUOTE_ATTRIBUTION)
            has_attr = (
                "\\begin{flushright}" in joined_tail
                or "\\QuoteAttribution" in joined_tail
                or any(marker and marker in joined_tail for marker in existing_markers)
            )

            if not has_attr:
                block.extend(quoted_snippet(attribution_text))

            out.extend(block)
            # ã“ã“ã§æ¬¡è¡Œã¯é '>'ï¼ˆblockquote ã®å¤–ï¼‰ãªã®ã§ã€ãã®ã¾ã¾ãƒ«ãƒ¼ãƒ—ç¶™ç¶š
            continue

        out.append(line)
        i += 1

    return "\n".join(out) + ("\n" if md_text.endswith("\n") else "")


def _find_closing_delimiter(text: str, start_idx: int, open_ch: str, close_ch: str) -> typing.Optional[int]:
    """start_idx ã‹ã‚‰å§‹ã¾ã‚‹æ‹¬å¼§ã®å¯¾å¿œä½ç½®ã‚’è¿”ã™ã€‚ãƒã‚¹ãƒˆã¨ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã‚’è€ƒæ…®ã€‚"""
    depth = 0
    i = start_idx
    n = len(text)
    while i < n:
        ch = text[i]
        if ch == "\\":
            i += 2
            continue
        if ch == open_ch:
            depth += 1
        elif ch == close_ch:
            depth -= 1
            if depth == 0:
                return i
        i += 1
    return None


def _strip_markdown_images(md_text: str) -> tuple[str, list[str]]:
    """Markdownã® ![]() / ![][] ç”»åƒã‚’å‰Šé™¤ã—ã€å‰Šé™¤å†…å®¹ã‚’è¿”ã™ã€‚"""
    removals: list[str] = []
    out: list[str] = []
    i = 0
    n = len(md_text)

    while i < n:
        ch = md_text[i]
        if (
            ch == "!"
            and (i == 0 or md_text[i - 1] != "\\")
            and i + 1 < n
            and md_text[i + 1] == "["
        ):
            alt_close = _find_closing_delimiter(md_text, i + 1, "[", "]")
            if alt_close is None:
                out.append(ch)
                i += 1
                continue
            alt_text = md_text[i + 2 : alt_close]
            j = alt_close + 1
            while j < n and md_text[j].isspace():
                j += 1
            if j < n and md_text[j] == "(":
                target_close = _find_closing_delimiter(md_text, j, "(", ")")
                if target_close is None:
                    out.append(ch)
                    i += 1
                    continue
                target = md_text[j + 1 : target_close].strip()
                summary = f"markdown:{alt_text.strip() or '(no alt)'} -> {target or '(empty)'}"
                removals.append(summary)
                i = target_close + 1
                continue
            if j < n and md_text[j] == "[":
                label_close = _find_closing_delimiter(md_text, j, "[", "]")
                if label_close is None:
                    out.append(ch)
                    i += 1
                    continue
                label = md_text[j + 1 : label_close].strip()
                summary = f"markdown_ref:{alt_text.strip() or '(no alt)'}[{label or '(implicit)'}]"
                removals.append(summary)
                i = label_close + 1
                continue
        out.append(ch)
        i += 1

    return "".join(out), removals


def strip_markdown_images_only(md_text: str) -> tuple[str, list[str]]:
    """Markdownç”»åƒè¨˜æ³•ã®ã¿ã‚’é™¤å»ã—ã€å‰Šé™¤ãƒ­ã‚°ã‚’è¿”ã™ã€‚"""
    return _strip_markdown_images(md_text)


def _log_sanitization(stage: str, source: Path, logs: list[str]) -> None:
    if not logs:
        return
    head = ", ".join(logs[:3])
    tail = "" if len(logs) <= 3 else f", ... (+{len(logs) - 3})"
    print(
        f"[info] Removed {len(logs)} embedded image snippet(s) for {stage}: {head}{tail} (source: {source})"
    )


def create_image_sanitized_copy(
    src_md_path: Path, suffix: str = ".no_images.md"
) -> tuple[Path, list[str]]:
    """
    Markdownã‚’èª­ã¿è¾¼ã¿ã€Markdownç”»åƒè¨˜æ³•ã‚’é™¤å»ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’
    /markdown_sanitized ã«ä¿å­˜ã—ã¦ãƒ‘ã‚¹ã¨å‰Šé™¤ãƒ­ã‚°ã‚’è¿”ã™ã€‚
    å‰Šé™¤å¯¾è±¡ãŒç„¡ã‘ã‚Œã°å…ƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’è¿”ã™ã€‚
    """
    text = src_md_path.read_text(encoding="utf-8")
    sanitized, logs = strip_markdown_images_only(text)
    if sanitized == text:
        return src_md_path, []

    sanitized_dir = src_md_path.parent / "markdown_sanitized"
    sanitized_dir.mkdir(parents=True, exist_ok=True)
    tmp_name = src_md_path.stem + suffix
    tmp_path = sanitized_dir / tmp_name
    tmp_path.write_text(sanitized, encoding="utf-8")
    return tmp_path, logs


def _find_front_matter_end(lines: list[str]) -> typing.Optional[int]:
    """YAMLãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼ãŒã‚ã‚Œã°çµ‚äº†è¡Œã®â€œæ¬¡â€ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¿”ã™ã€‚"""
    if not lines:
        return None
    if lines[0].strip() != "---":
        return None
    for idx in range(1, len(lines)):
        stripped = lines[idx].strip()
        if stripped in {"---", "..."}:
            return idx + 1
    return None


def _normalize_horizontal_rules_for_pdf(md_text: str) -> str:
    """
    Pandocã® markdown+yaml_metadata_block ã§ã¯ã€æ–‡ä¸­ã® '---' å˜ç‹¬è¡ŒãŒ
    YAMLãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦èª¤è§£ã•ã‚Œã†ã‚‹ãŸã‚ã€PDFç”¨ãƒ†ãƒ³ãƒãƒ©ãƒªã§ã¯ '***' ã«ç½®æ›ã™ã‚‹ã€‚
    â€»ãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼ï¼ˆå…ˆé ­ã® --- ... --- ï¼‰ã¯ä¿æŒã™ã‚‹ã€‚
    """
    lines = md_text.splitlines(keepends=False)
    front_matter_end = _find_front_matter_end(lines)
    if front_matter_end is None:
        front_matter_end = 0
    prefix = lines[:front_matter_end]
    body = lines[front_matter_end:]

    normalized_body: list[str] = []
    for line in body:
        stripped = line.strip()
        if stripped and set(stripped) <= {"-"} and len(stripped) >= 3:
            leading = line[: len(line) - len(line.lstrip())]
            normalized_body.append(f"{leading}***")
        else:
            normalized_body.append(line)

    merged = prefix + normalized_body
    return "\n".join(merged) + ("\n" if md_text.endswith("\n") else "")


def add_attribution_to_blockquotes_file(
    src_md_path: Path,
    attribution_text: str,
    suffix: str = ".with_attrib.md",
) -> tuple[Path, list[str]]:
    """
    å…¥åŠ› Markdown ã‚’èª­ã¿ã€Markdownãƒ•ã‚¡ã‚¤ãƒ«å†’é ­ã«ç”»åƒã«ã¤ã„ã¦ã®æ³¨æ„æ›¸ãã‚’è¿½åŠ ã€‚
    ã•ã‚‰ã«ã€blockquote æœ«å°¾ã«è„šæ³¨ã‚’è¿½åŠ ã—ãŸåŠ å·¥ Markdown ã‚’
    ãƒ•ã‚¡ã‚¤ãƒ«: /markdown_with_attrib ã«ä¿å­˜ã—ã¦ãƒ‘ã‚¹ã‚’è¿”ã™ã€‚
    """
    src_md_path = Path(src_md_path)
    try:
        text = src_md_path.read_text(encoding="utf-8")
    except Exception as e:
        logger.exception("Markdownèª­ã¿è¾¼ã¿ã«å¤±æ•—: %s", src_md_path)
        raise

    logs: list[str] = []
    try:
        injected = _inject_attribution_to_blockquotes(text, attribution_text)
        new_text = _normalize_horizontal_rules_for_pdf(injected)
        # ã•ã‚‰ã«å†’é ­ã«æ³¨æ„æ›¸ãã‚’è¿½åŠ 
        citation = "â€»ç”»åƒã®èª­è§£ã«ã¤ã„ã¦ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ç‰¹æ€§ä¸Šã€å®Ÿéš›ã®æ‰€è¦‹ã¨ç•°ãªã‚‹è§£é‡ˆã‚„ä¸æ­£ç¢ºãªèª¬æ˜ãŒå‡ºåŠ›ã•ã‚Œã‚‹ãƒªã‚¹ã‚¯ãŒã”ã–ã„ã¾ã™ã€‚è‡¨åºŠåˆ¤æ–­ãƒ»æ•™è‚²è©•ä¾¡ãƒ»å…¬å¼æ–‡æ›¸ç­‰ã¸ã®è»¢ç”¨ã«éš›ã—ã¦ã¯ã€å¿…ãšåŸè³‡æ–™ãŠã‚ˆã³ä¸€æ¬¡æƒ…å ±ã‚’å†ç¢ºèªã—ã€å°‚é–€å®¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’çµŒãŸä¸Šã§æ…é‡ã«ã”åˆ©ç”¨ãã ã•ã„ã€‚"
        # å†’é ­ã«æ³¨æ„æ›¸ããŒè¿½åŠ ã•ã‚Œã¦ã„ãªã„å ´åˆã®ã¿è¿½åŠ 
        if citation not in new_text:
            new_text = f"**{citation}**\n\n" + new_text

        sanitized_text, logs = strip_markdown_images_only(new_text)
        sanitized_text = sanitize_symbols_v36(sanitized_text)

    except Exception as e:
        logger.exception("blockquote è„šæ³¨ã®è‡ªå‹•è¿½è¨˜ã§ã‚¨ãƒ©ãƒ¼: %s", e)
        raise

    # ãƒ•ã‚©ãƒ«ãƒ€ï¼š markdown_with_attrib ãŒãªã‘ã‚Œã°ä½œæˆ
    attrib_dir = src_md_path.parent / "markdown_with_attrib"
    try:
        attrib_dir.mkdir(parents=True, exist_ok=True)
    except Exception as e:
        logger.exception("è„šæ³¨ä»˜ã Markdown ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆã«å¤±æ•—: %s", attrib_dir)
        raise
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–°è¦ä½œæˆã—ã¦ä¿å­˜ï¼ˆã‚ªãƒªã‚¸ãƒŠãƒ«ã¯å¤‰æ›´ã—ãªã„ï¼‰
    tmp_name = src_md_path.stem + suffix
    tmp_path = attrib_dir / tmp_name
    try:
        tmp_path.write_text(sanitized_text, encoding="utf-8")
    except Exception as e:
        logger.exception("åŠ å·¥ Markdown ã®æ›¸ãå‡ºã—ã«å¤±æ•—: %s", tmp_path)
        raise

    return tmp_path, logs


def _build_pandoc_env(tmp_base: pathlib.Path) -> dict:
    """pandocç”¨ã«å®‰å…¨ãªä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç’°å¢ƒå¤‰æ•°ã§æŒ‡å®šã™ã‚‹ã€‚
    macOSã§TMPDIRãŒ /private/var/folders/zz/zyxvpxvq6csfxvn_n0000000000000/T ã®ã‚ˆã†ãª
    ã‚°ãƒ­ãƒ¼ãƒãƒ«é ˜åŸŸã‚’æŒ‡ã™ã¨ã€createDirectory: permission denied ãŒèµ·ã“ã‚Šã†ã‚‹ãŸã‚ã€
    å‡ºåŠ›ãƒ™ãƒ¼ã‚¹ç›´ä¸‹ã«å°‚ç”¨tmpã‚’ä½œã£ã¦å›ºå®šã™ã‚‹ã€‚
    """
    env = os.environ.copy()
    try:
        tmp_base.mkdir(parents=True, exist_ok=True)
    except Exception:
        # ä½œæˆã«å¤±æ•—ã—ã¦ã‚‚ä»¥é™ã®subprocessã§ä¸Šæ›¸ãã•ã‚Œã‚‹ã ã‘ãªã®ã§æ¡ã‚Šã¤ã¶ã™
        pass
    env["TMPDIR"] = str(tmp_base)
    env["TMP"] = str(tmp_base)
    env["TEMP"] = str(tmp_base)
    return env


def _normalize_stem(stem: str) -> str:
    """"_è§£ç­”è§£èª¬"ãŒå«ã¾ã‚Œã¦ã„ãªã‘ã‚Œã°ä»˜ä¸ã—ã¦é‡è¤‡ã‚’é¿ã‘ã‚‹ã€‚"""
    return stem if "_è§£ç­”è§£èª¬" in stem else f"{stem}_è§£ç­”è§£èª¬"


def _calc_output_root_for_input_path(p: pathlib.Path) -> pathlib.Path:
    """å…¥åŠ›ãŒãƒ•ã‚¡ã‚¤ãƒ«ãªã‚‰è¦ªã®è¦ªã€ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãªã‚‰è¦ªã‚’å‡ºåŠ›ãƒ™ãƒ¼ã‚¹ã¨ã™ã‚‹ã€‚"""
    if p.is_dir():
        return p.parent
    gp = p.parent.parent
    # ãƒ«ãƒ¼ãƒˆç›´ä¸‹ãªã©ã§è¦ªã®è¦ªãŒåŒä¸€ã¨ãªã‚‹å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    return gp if gp != p.parent else p.parent


def convert_one(filepath: pathlib.Path, output_root: pathlib.Path) -> None:
    """å˜ä¸€Markdownã‚’docx/pdfã¸å¤‰æ›ã™ã‚‹ã€‚å‡ºåŠ›ã¯ output_root/{docx,pdf} é…ä¸‹ã€‚"""
    pandoc_env = _build_pandoc_env(output_root / ".pandoc-tmp")
    original_md_path = Path(filepath)
    md_for_pdf_path: Path = original_md_path
    attribution_text, attr_source, attr_yaml_path, missing_keys = resolve_blockquote_attribution(original_md_path)

    if attr_source == "environment":
        print(f"[info] BLOCKQUOTE_ATTRIBUTION ã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ä½¿ç”¨ã—ã¾ã™: {attribution_text}")
    elif attr_source == "metadata":
        source_note = str(attr_yaml_path) if attr_yaml_path else "metadata"
        if missing_keys:
            missing = "ã€".join(missing_keys)
            print(f"[info] BLOCKQUOTE_ATTRIBUTION ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ ({source_note}) ã‹ã‚‰å–å¾—: {attribution_text} ï¼ˆæ¬ æ: {missing} â†’ \"ä¸æ˜\" ã¨ã—ã¦åˆ©ç”¨ï¼‰")
        else:
            print(f"[info] BLOCKQUOTE_ATTRIBUTION ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ ({source_note}) ã‹ã‚‰å–å¾—: {attribution_text}")
    else:
        print(f"[warn] BLOCKQUOTE_ATTRIBUTION ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚‰ãšæ—¢å®šå€¤ã‚’ä½¿ç”¨ã—ã¾ã™: {attribution_text}")

    docx_input_path = original_md_path
    docx_logs: list[str] = []
    try:
        docx_input_path, docx_logs = create_image_sanitized_copy(
            original_md_path, suffix=".docx.no_images.md"
        )
    except Exception as exc:
        print(f"[warn] docxç”¨ã®ç”»åƒé™¤å»ã«å¤±æ•—ã€‚å…ƒã®Markdownã‚’ä½¿ç”¨ã—ã¾ã™: {exc}")
        docx_input_path = original_md_path
        docx_logs = []
    _log_sanitization("docx", original_md_path, docx_logs)

    # Word(docx)
    try:
        docx_output_dir = output_root / "docx"
        docx_output_dir.mkdir(parents=True, exist_ok=True)
        base = _normalize_stem(filepath.stem)
        docx_output_path = docx_output_dir / f"{base}.docx"
        subprocess.run(
            [
                "pandoc",
                str(docx_input_path),
                "-f",
                DOCX_PANDOC_INPUT_FORMAT,
                "-o",
                str(docx_output_path),
            ],
            check=True,
            env=pandoc_env,
        )
        print(f"Converted to Word document: {docx_output_path}")
        try:
            md_for_pdf_path, pdf_logs = add_attribution_to_blockquotes_file(
                src_md_path=original_md_path,
                attribution_text=attribution_text,
                suffix=".with_attrib.md",
            )
            _log_sanitization("pdf", original_md_path, pdf_logs)
            print(f"[info] blockquote è„šæ³¨ã‚’ä»˜ä¸ã—ãŸ Markdown ã‚’ç”Ÿæˆ: {md_for_pdf_path.name}")
        except Exception as e:
            print(f"[warn] blockquote è„šæ³¨ä»˜ä¸ã«å¤±æ•—ã€‚å…ƒã® Markdown ã§ç¶™ç¶šã—ã¾ã™: {e}")
            md_for_pdf_path = docx_input_path
    except subprocess.CalledProcessError:
        print("pandoc command failed. Please make sure pandoc is installed.")
    except FileNotFoundError:
        print("pandoc not found. Please install pandoc first.")
        print("Install with: brew install pandoc (macOS) or apt install pandoc (Ubuntu)")

    # PDF
    try:
        pdf_output_dir = output_root / "pdf"
        pdf_output_dir.mkdir(parents=True, exist_ok=True)
        base = _normalize_stem(filepath.stem)
        pdf_output_path = pdf_output_dir / f"{base}.pdf"
        subprocess.run(
            [
                "pandoc",
                str(md_for_pdf_path),
                "-f",
                PDF_PANDOC_INPUT_FORMAT,
                "-o",
                str(pdf_output_path),
                "--pdf-engine=lualatex",
                "-V",
                "documentclass=ltjsarticle",
                "--include-in-header=header-lua.tex",
                "--include-in-header=header-quote-bg.tex",
            ],
            check=True,
            env=pandoc_env,
        )
        print(f"Converted to PDF document: {pdf_output_path}")
    except subprocess.CalledProcessError:
        print("pandoc command failed. Please make sure pandoc is installed.")
    except FileNotFoundError:
        print("pandoc not found. Please install pandoc first.")
        print("Install with: brew install pandoc (macOS) or apt install pandoc (Ubuntu)")


if len(sys.argv) < 2:
    print(
        "Usage: python3 convert_md_to_pdfs.py <markdown_file|directory|files...> "
        "å„å…¥åŠ›ã«å¯¾ã—ã¦ã€å‡ºåŠ›å…ˆã¯ markdownãƒ•ã‚¡ã‚¤ãƒ«=ãã®è¦ªã®è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª=ãã®è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª ã®ç›´ä¸‹ã« /docx ã¨ /pdf ã‚’ä½œæˆã—ã¾ã™ã€‚"
    )
    sys.exit(1)

args = sys.argv[1:]
candidate_paths = [pathlib.Path(a) for a in args]

tasks: list[tuple[pathlib.Path, pathlib.Path]] = []
for p in candidate_paths:
    if p.is_dir():
        out_root = _calc_output_root_for_input_path(p)
        md_list = sorted(p.glob("*.md"))
        for md in md_list:
            tasks.append((md, out_root))
    elif p.is_file():
        if p.suffix.lower() in {".md", ".markdown", ".mdown"}:
            out_root = _calc_output_root_for_input_path(p)
            tasks.append((p, out_root))
        else:
            print(f"[skip] Not a markdown file: {p}")
    else:
        print(f"[warn] Not found: {p}")

if not tasks:
    print("[error] No markdown files to convert. æŒ‡å®šã—ãŸãƒ‘ã‚¹ã« .md ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    sys.exit(1)

for md, out_root in tasks:
    print(f"[convert] {md} -> {out_root}")
    convert_one(md, out_root)

# Changelog
# 2025-10-01: ver1.0 Initial version (run_pipeline.py-v5.8ã‹ã‚‰åˆ†å‰²ã—ã¦ä½œæˆ)
# 2025-10-01: ver1.1 ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§æŒ‡å®šå¯èƒ½ã«å¤‰æ›´
# 2025-10-03: ver1.2 ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª/è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«/ãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰å…¥åŠ›ã«å¯¾å¿œã€‚æœ«å°¾å¼•æ•°ãŒæ—¢å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å ´åˆã¯å‡ºåŠ›ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ‰±ã†ã€‚"_è§£ç­”è§£èª¬" ã®é‡è¤‡ä»˜ä¸ã‚’æŠ‘æ­¢ã€‚
# 2025-10-03: ver1.3 macOSã®TMPDIRæ¨©é™å•é¡Œã«å¯¾å‡¦ã€‚pandocå®Ÿè¡Œæ™‚ã®ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ output_root/.pandoc-tmp ã«å›ºå®šã—ã¦ Permission denied ã‚’å›é¿ï¼ˆpandocã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ä¸å¤‰æ›´ï¼‰ã€‚
# 2025-10-03: ver1.4 å¼•æ•°ã‚’å…¥åŠ›ãƒ‘ã‚¹ç¾¤ã®ã¿(<markdown_file|directory|files...>)ã«ç°¡ç´ åŒ–ã€‚å‡ºåŠ›å…ˆã¯ file=è¦ªã®è¦ªã€dir=è¦ª ã¨ã—ã€ãã‚Œãã‚Œç›´ä¸‹ã« /docx ã¨ /pdf ã‚’ä½œæˆï¼ˆpandocã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ä¸å¤‰æ›´ï¼‰ã€‚
# 2025-10-19: ver1.7 æ–°ãŸã«å¼•ç”¨æ–‡ã«å¯¾ã—ã¦èƒŒæ™¯ã‚’å¤‰æ›´ã—ã¦è¦‹ã‚„ã™ãã™ã‚‹ã‚ˆã†ãª .txtãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã— ã•ã‚‰ã«å¼•ç”¨æ–‡ã®æœ«å°¾ã« æ©Ÿæ¢°çš„ã«å®¢æ³¨ã‚’ã¤ã‘ã‚‹ã‚ˆã†ãª ã‚³ãƒ¼ãƒ‰ã‚’è¿½åŠ 
# 2025-10-19: ver1.8 PDFç”Ÿæˆç”¨ã®pandocå…¥åŠ›ã‚’ raw_tex å¯èƒ½ãªMarkdownæ‹¡å¼µã«åˆ‡ã‚Šæ›¿ãˆã€blockquoteè„šæ³¨ã®è‡ªå‹•è¿½è¨˜ã‚’å®‰å®šåŒ–
# 2025-10-19: ver1.9 PDFç”¨ãƒ†ãƒ³ãƒãƒ©ãƒªã§ '---' æ°´å¹³ç·šã‚’ '***' ã«å¤‰æ›ã— YAMLèª¤æ¤œçŸ¥ã‚’å›é¿
# 2025-10-20: ver2.0 blockquoteè„šæ³¨ä»˜ãMarkdownã®ä¿å­˜å…ˆã‚’ /markdown_with_attrib ã«å¤‰æ›´ ãªã‘ã‚Œã°ä½œæˆ
# 2025-10-21: ver2.1 ã‚µã‚¤ãƒ‰ã‚«ãƒ¼YAMLã‹ã‚‰å¤§å­¦åãƒ»å¹´åº¦ãƒ»è©¦é¨“ç§‘ç›®ã‚’å–å¾—ã—ã¦è„šæ³¨ãƒ©ãƒ™ãƒ«ã‚’è‡ªå‹•ç”Ÿæˆã—ã€æ¬ ææ™‚ã¯ã€Œä¸æ˜ã€ã§ç¶™ç¶š
# 2025-10-23: ver2.2 +hard_line_breaks ã‚’PDFç”¨pandocå…¥åŠ›ã«è¿½åŠ  â†’ æ”¹è¡Œã®æ‰±ã„ã‚’GFMæº–æ‹ ã«æ”¹å–„
# 2025-10-24: ver2.3 å†’é ­ã«ç”»åƒã«ã¤ã„ã¦ã®æ³¨æ„æ›¸ãã‚’è¿½åŠ  â†’ å¤ªå­—ã«å¤‰æ›´
# 2025-10-24: ver2.4 DOCX/PDFå…¥åŠ›ã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚ºã—ã¦ç”»åƒãƒ»HTMLãƒ»\\includegraphicsã‚’ç„¡è¦–ã—ã€æ¤œå‡ºãƒ­ã‚°ã‚’å‡ºåŠ›ã€‚pandocå…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‹ã‚‰ raw_html ã‚’ç„¡åŠ¹åŒ–ã€‚
# 2025-11-08: ver2.5 ã‚µãƒ‹ã‚¿ã‚¤ã‚ºå¯¾è±¡ã‚’Markdownç”»åƒè¨˜æ³•ã®ã¿ã«é™å®šã—ã€HTMLãƒ†ã‚­ã‚¹ãƒˆã®èª¤å‰Šé™¤ã‚’é˜²æ­¢ã€‚
# 2025-11-08: ver2.6 ãƒ­ã‚°å‡ºåŠ›ã‚’è‹¥å¹²æ”¹å–„ã€‚
# 2025-12-16: ver3.5 weasyprint å»ƒæ­¢ã«ä¼´ã„ä»¥å‰ã® pandoc + lualatex ã«ã‚ˆã‚‹ PDF ç”Ÿæˆã«æˆ»ã™ã€‚
# 2025-12-16: ver3.6 PDFç”¨Markdownã®æ›¸ãå‡ºã—æ™‚ã«ç‰¹å®šè¨˜å·ã®ASCIIç½®æ›ã¨ã‚µãƒ­ã‚²ãƒ¼ãƒˆãƒšã‚¢é™¤å»ã‚’è¿½åŠ ã—ã€pandoc/pdftexã®è­¦å‘Šã‚’æŠ‘åˆ¶ã€‚
